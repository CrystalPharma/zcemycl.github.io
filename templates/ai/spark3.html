<!DOCTYPE html>
<html>
<head>
    <meta content="text/html;charset=utf-8" http-equiv="Content-Type">
    <meta content="utf-8" http-equiv="encoding">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <meta name="viewport" content="width=device-width; initial-scale=1.0">
    <style>
      body{background-color:black;color:white;height:100%;font-family:Arial,Helvetica,sans-serif;}
      /*h3{color:#00ff2b}*/
      a{color:#337AB7;font-weight:bold;text-decoration:none;}
      pre code {
        color:white;
        border: 1px solid #999;
        display: block;
        padding: 5px 10px;
        white-space:pre-wrap;
        word-wrap:break-word;

      }
      table{width:100%}
      td,tr,th{border:1px solid white;}
      td,th{width:25%;}
      div.formula{overflow-x:scroll;border:1px solid white;}
    ::-webkit-scrollbar {
        width: 5px;
    }

    /* Track */
    ::-webkit-scrollbar-track {
        width:5px;
        -webkit-box-shadow: inset 0 0 6px rgba(0,0,0,0.3);
        -webkit-border-radius: 10px;
        border-radius: 10px;
    }

    /* Handle */
    ::-webkit-scrollbar-thumb {
        -webkit-border-radius: 10px;
        width:5px;
        border-radius: 10px;
        background: #555;
        -webkit-box-shadow: inset 0 0 6px rgba(0,0,0,0.5);
    }

    /* Handle on hover */
    ::-webkit-scrollbar-track:hover {
        width:5px;
        background: #555;
    }

    ::-webkit-scrollbar-thumb:hover {
        width:5px;
        background: orange;
    }

    /* Handle on hover */
    ::-webkit-scrollbar-track:active {
        width:5px;
        background: #555;
    }

    </style>
    <script src="https://unpkg.com/axios/dist/axios.min.js"></script>
</head>
<body class="content wrapper">
    <div style="background-color:black; color:white; padding:0 2% 0 2%; height: 100%; flex:1;overflow: auto;">

    <h3>Spark with Pi Cluster 3</h3>
    <hr>

    <h4>Java Installation</h4>
    <p>This can be done for all nodes.</p>
    <pre><code>
clustercmd 'sudo apt install -y default-jre > /dev/null 2>&1'
clustercmd java -version
clustercmd 'sudo apt install -y default-jdk > /dev/null 2>&1'
clustercmd df -h
clustercmd 'sudo apt install -y scala > /dev/null 2>&1'
clustercmd scala -version
echo 'export JAVA_HOME=$(readlink -f /usr/bin/java | sed "s:bin/java::")' >> ~/.bashrc
    </code></pre>

    <p>The following sections are only applied to first Node (master node/ name node). Other datanodes can be done in later article.</p>
    <h4>Hadoop Installation</h4>
    <p>Download the correct tar file, by clicking Announcement under Release notes &#8594 Download tar.gz (not src!!)</p>
    <pre><code>
scp hadoop-3.3.1.tar.gz pi41:
sudo tar -xvf hadoop-3.3.1.tar.gz -C /opt/
sudo mkdir hadoop
sudo mv hadoop-3.3.1/* hadoop
sudo rm -r hadoop-3.3.1
sudo chown pi:pi -R /opt/hadoop
echo 'export HADOOP_HOME=/opt/hadoop' >> ~/.bashrc
echo 'export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin' >> ~/.bashrc
# edit /opt/hadoop/etc/hadoop/hadoop-env.sh to add this line
export JAVA_HOME=$(readlink –f /usr/bin/java | sed "s:bin/java::") 
echo 'export JAVA_HOME=$(readlink –f /usr/bin/java | sed "s:bin/java::")' >> /opt/hadoop/etc/hadoop/hadoop-env.sh

source ~/.bashrc
    </code></pre>
    <p>Test with,</p>
    <pre><code>
cd && hadoop version | grep Hadoop
    </code></pre>


    <h4>Spark Installation</h4>
    <p>Download the correct tar file under Download Apache Spark &#8594 Step 3.</p>
    <pre><code>
scp spark-3.1.2-bin-hadoop3.2.tgz pi41:
sudo tar -xvf spark-3.1.2-bin-hadoop3.2.tgz -C /opt/
cd /opt && sudo mkdir spark
sudo mv spark-3.1.2-bin-hadoop3.2/* spark
sudo rm -r spark-3.1.2-bin-hadoop3.2
sudo chown pi:pi -R /opt/spark
echo 'export SPARK_HOME=/opt/spark' >> ~/.bashrc
echo 'export PATH=$PATH:$SPARK_HOME/bin' >> ~/.bashrc
source ~/.bashrc
    </code></pre>
    <p>Test with,</p>
    <pre><code>
spark-shell --version
    </code></pre>


    <h4>Configure HDFS</h4>
    <p>Please configure these in Pi1 as master node, all files are within <code>/opt/hadoop/etc/hadoop</code>, </p>
    <h5><b>core-site.xml</b></h5>
    <pre><code>
&ltconfiguration&gt
  &ltproperty&gt
    &ltname&gtfs.defaultFS&lt/name&gt
    &ltvalue&gthdfs://pi41:9000&lt/value&gt
  &lt/property&gt
&lt/configuration&gt
    </code></pre>


    <h5><b>hdfs-site.xml</b></h5>
    <pre><code>
&ltconfiguration&gt
  &ltproperty&gt
    &ltname&gtdfs.datanode.data.dir&lt/name&gt
    &ltvalue&gtfile:///opt/hadoop_tmp/hdfs/datanode&lt/value&gt
  &lt/property&gt
  &ltproperty&gt
    &ltname&gtdfs.namenode.name.dir&lt/name&gt
    &ltvalue&gtfile:///opt/hadoop_tmp/hdfs/namenode&lt/value&gt
  &lt/property&gt
  &ltproperty&gt
    &ltname&gtdfs.replication&lt/name&gt
    &ltvalue&gt1&lt/value&gt
  &lt/property&gt
&lt/configuration&gt
    </code></pre>
    <p>Run the following commands to refer the change,</p>
    <pre><code>
sudo mkdir -p /opt/hadoop_tmp/hdfs/datanode
sudo mkdir -p /opt/hadoop_tmp/hdfs/namenode
sudo chown pi:pi -R /opt/hadoop_tmp
    </code></pre>


    <h5><b>mapred-site.xml</b></h5>
    <pre><code>
&ltconfiguration&gt
  &ltproperty&gt
    &ltname&gtmapreduce.framework.name&lt/name&gt
    &ltvalue&gtyarn&lt/value&gt
  &lt/property&gt
&lt/configuration&gt
    </code></pre>

    <h5><b>yarn-site.xml</b></h5>
    <pre><code>
&ltconfiguration&gt
  &ltproperty&gt
    &ltname&gtyarn.nodemanager.aux-services&lt/name&gt
    &ltvalue&gtmapreduce_shuffle&lt/value&gt
  &lt/property&gt
  &ltproperty&gt
    &ltname&gtyarn.nodemanager.auxservices.mapreduce.shuffle.class&lt/name&gt  
    &ltvalue&gtorg.apache.hadoop.mapred.ShuffleHandler&lt/value&gt
  &lt/property&gt
&lt/configuration&gt
    </code></pre>

    <p>Test with,</p>
    <pre><code>
hdfs namenode -format -force # only do this at the first time, otherwise it will cause data lost in hdfs, to fix it see Troubleshooting section.
start-dfs.sh
start-yarn.sh 
jps # must show NameNode, DataNode, NodeManager, ResourceManager, SecondaryNameNode, Jps. Otherwise, see Troubleshooting section.
    </code></pre>

    <p>Basic commands to control file system (fs), they look like extra layer to do linux bash,</p>
    <pre><code>
hadoop fs -mkdir /tmp
hadoop fs -ls /
#### Continue to test ####
hadoop fs -put $SPARK_HOME/README.md / 
spark-shell # start spark host
scala&gt val textFile = sc.textFile("hdfs://pi41:9000/README.md")
...

scala&gt textFile.first()
res0: String = # Apache Spark 
    </code></pre>

    <p>In your browser, type in <code>http://{pi41_ip}:4040</code> or <code>http://{pi41_ip}:8088</code>, you should see the spark console as follows,</p>
    <div style="text-align:center">
      <img src="../../resources/spark.png" style="width:30%">
      <img src="../../resources/hadoop2.png" style="width:30%">
      <figcaption>You can see the textfile command is listed as completed jobs.</figcaption>
    </div>
    

    <h4>Troubleshootings</h4>
    <ol>
      <li>Hide <code>execstack</code> Warning, i.e. <code>You have loaded library... which might have disabled stack guard.</code></li>
      <pre><code>
# edit /opt/hadoop/etc/hadoop/hadoop-env.sh
export HADOOP_OPTS="-XX:-PrintWarnings –Djava.net.preferIPv4Stack=true"
      </code></pre>
      <li>Hide <code>NativeCodeLoader</code> Warning, i.e. <code>Unable to load [the] native-hadoop library for your platform</code>, </li>
      <pre><code>
echo 'export HADOOP_HOME_WARN_SUPPRESS=1' >> ~/.bashrc
echo 'export HADOOP_ROOT_LOGGER="WARN,DRFA"' >> ~/.bashrc
source ~/.bashrc
      </code></pre>
      <li>Restart properly, </li>
      <pre><code>
# Method 1 reboot, then run start-xxx.sh commands
# Method 2 
stop-all.sh && start-dfs.sh && start-yarn.sh
      </code></pre>
      <li>Missing datanode by <code>jps</code> and eroor <code>Hadoop java io IOException File could only be replicated to 0 nodes instead of minReplication 1 </code> when running <code>hadoop fs -put $SPARK_HOME/README.md /</code>, caused by accidentally command <code>hdfs namenode -format -force</code> again,</li>
      <pre><code>
sudo rm -r /opt/hadoop_tmp
sudo mkdir -p /opt/hadoop_tmp/hdfs/datanode
sudo mkdir -p /opt/hadoop_tmp/hdfs/namenode
sudo chown pi:pi -R /opt/hadoop_tmp
# Now rerun everything again,
hdfs namenode -format -force
start-dfs.sh
start-yarn.sh # show work
      </code></pre>
      <li></li>
    </ol>


    <h3>References</h3>
    <hr>
    <ol>
      <li><a href="https://towardsdatascience.com/assembling-a-personal-data-science-big-data-laboratory-in-a-raspberry-pi-4-or-vms-cluster-ff37759cb2ec">A Data Science/Big Data Laboratory — part 1 of 4: Raspberry Pi or VMs cluster — OS and communication</a></li>
      <li><a href="https://towardsdatascience.com/assembling-a-personal-data-science-big-data-laboratory-in-a-raspberry-pi-4-or-vms-cluster-e4c5a0473025">A Data Science/Big Data Laboratory — part 2 of 4: Hadoop 3.2.1 and Spark 3.0.0 over Ubuntu 20.04 in a 3-node cluster</a></li>
      <li><a href="https://dev.to/awwsmm/building-a-raspberry-pi-hadoop-spark-cluster-8b2">Building a Raspberry Pi Hadoop / Spark Cluster </a></li>
      <li><a href="https://dev.to/awwsmm/installing-and-running-hadoop-and-spark-on-ubuntu-18-393h">Installing and Running Hadoop and Spark on Ubuntu 18 </a></li>
      <li><a href="https://medium.com/analytics-vidhya/build-raspberry-pi-hadoop-spark-cluster-from-scratch-c2fa056138e0">Build Raspberry Pi Hadoop/Spark Cluster from scratch</a></li>
      <li><a href="https://sparkbyexamples.com/pyspark-tutorial/">Spark with Python (PySpark) Tutorial For Beginners</a></li>
      <li><a href="https://hadoop.apache.org/releases.html">Download Apache Hadoop</a></li>
      <li><a href="https://spark.apache.org/downloads.html">Download Apache Spark</a></li>
      <li><a href="https://codetober.com/raspberry-pi-4-how-to-install-apache-spark/">Raspberry Pi 4: How To Install Apache Spark</a></li>
      <li><a href="https://scott-ralph.medium.com/creating-an-apache-spark-cluster-with-raspberry-pi-workers-472c2c9a19ce">Creating an Apache Spark Cluster with Raspberry Pi Workers</a></li>
      <li><a href="https://medium.com/data-waffles/raspberry-pi-hadoop-cluster-guide-2559437d232">Raspberry Pi Hadoop Cluster Guide</a></li>
      <li><a href=""></a></li>
    </ol>
    </div>
    <script type="text/javascript" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
    <script type="text/javascript" src="https://zcemycl.github.io/static/eqresp.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
    <script>mermaid.initialize({startOnLoad:true});</script>
    <script>
    </script>
    
</body>
</html>
