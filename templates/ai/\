<!DOCTYPE html>
<html>
<head>
    <meta content="text/html;charset=utf-8" http-equiv="Content-Type">
    <meta content="utf-8" http-equiv="encoding">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <meta name="viewport" content="width=device-width; initial-scale=1.0">
    <style>
      body{background-color:black;color:white;height:100%;font-family:Arial,Helvetica,sans-serif;}
      /*h3{color:#00ff2b}*/
      a{color:#337AB7;font-weight:bold;text-decoration:none;}
      pre code {
        color:white;
        border: 1px solid #999;
        display: block;
        padding: 5px 10px;
        white-space:pre-wrap;
        word-wrap:break-word;

      }
      table{width:100%}
      td,tr,th{border:1px solid white;}
      td,th{width:25%;}
      div.formula{overflow-x:scroll;border:1px solid white;}

    </style>
</head>
<body class="content wrapper">
    <div style="background-color:black; color:white; padding:0 2% 0 2%; height: 100%; flex:1;overflow: auto;">

    <h3>Policy Iteration</h3>
    <hr>
    <h4>Value Functions</h4>
    <p>Value functions (of states/state-action pairs) are to estimate how good it is for the agent to be in a given state (/to perform a given action and state).</p>
    $$
    \begin{align*}
    v_\pi(s)&=\mathbb{E}_\pi[G_t|S_t=s]=\mathbb{E}_\pi\big[\sum^\infty_{k=0}\gamma^kR_{t+k+1}|S_t=s\big] \\
    &=\mathbb{E}_\pi\big[R_{t+1}+\gamma\sum^\infty_{k=0}\gamma^kR_{t+k+2}|S_t=s\big]\\
    &=\sum_a\pi(a|s)\sum_{s'}\sum_rp(s',r|s,a)\bigg[r+\gamma\mathbb{E}_\pi\big[\sum^\infty_{k=0}\gamma_kR_{t+k+2}|S_{t+1}=s'\big]\bigg]\\
    &=\sum_a\pi(a|s)\sum_{s',r}p(s',r|s,a)[r+\gamma v_\pi(s')]
    \end{align*}
    $$
    <p>Interestingly, you can see the value of a state given a policy is the estimate of the future state value. This represents the value of the current state is related to the future states, not the previous states. This is the Bellman equation for \(v_\pi\).</p>
    <p>The expected return starting from s, taking the action a, and following policy \(\pi\).</p>
    $$q_\pi(s,a)=\mathbb{E}_\pi[G_t|S_t=s,A_t=a]=\mathbb{E}_\pi\big[\sum^\infty_{k=0}\gamma^kR_{t+k+1}|S_t=s,A_t=a\big]\\
    $$
    <h4></h4>
    <p></p>
    <div style="text-align:center">
    <img src="https://raw.githubusercontent.com/zcemycl/Robotics/master/Reinforcemnet%20Learning/ValIter2.png" width="30%">
    <figcaption>Grid world</figcaption>
    </div>
    <h4>Steps</h4>
    <ul>
    <li>Initialize values and policies arbitrarily.</li>
    <li>Update values and policies iteratively,</li>
    $$
    \begin{align*}
    V_{k+1}(s)&=\sum_a\pi(a|s)\sum_{s',r}p(s',r|s,a)(r+\gamma V_k(s')) \\
    \pi'(s) &= \arg\max_a\sum_{s',r}p(s',r|s,a)(r+\gamma V_\pi(s'))\\
    \end{align*}
    $$
    <li>Criterions:
        <ul>
        <li>Policy Evaluation</li>
        $$\max_s|V_{k+1}(s)-V_k(s)|<\theta$$
        <li>Policy Improvement</li>
        $$V_{\pi'}(s)\geq V_\pi(s)$$
        </ul>
    </li>
    </ul>
    <h3>References</h3>
    <hr>
    </div>
    <script type="text/javascript" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
    <script type="text/javascript" src="https://zcemycl.github.io/static/eqresp.js"></script>

    
</body>
</html>
