<!DOCTYPE html>
<html>
<head>
    <meta content="text/html;charset=utf-8" http-equiv="Content-Type">
    <meta content="utf-8" http-equiv="encoding">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <meta name="viewport" content="width=device-width; initial-scale=1.0">
    <style>
      body{background-color:black;color:white;height:100%;font-family:Arial,Helvetica,sans-serif;}
      /*h3{color:#00ff2b}*/
      a{color:#337AB7;font-weight:bold;text-decoration:none;}
      pre code {
        color:white;
        border: 1px solid #999;
        display: block;
        padding: 5px 10px;
        white-space:pre-wrap;
        word-wrap:break-word;

      }
      table{width:100%}
      td,tr,th{border:1px solid white;}
      td,th{width:25%;}
      div.formula{overflow-x:scroll;border:1px solid white;}

    </style>
</head>
<body class="content wrapper">
    <div style="background-color:black; color:white; padding:0 2% 0 2%; height: 100%; flex:1;overflow: auto;">

    <h3>Monte Carlo Methods</h3>
    <hr>
    <p>Sometimes it is impossible to assume complete knowledge of the environment. Monte Carlo methods require only experience, including sample sequences of states, actions, and rewards from actual or simulated interaction with an environment.</p>
    <p>A model is still required, but it only needs to generate sample transitions, not the complete probability distributions of all possible transitions that is required for DP. This means, mcm does not need transition probabilities, like \(p(s',r|s,a)\).</p>
    <p>Monte Carlo Methods (MCM) sample and average returns for each state-action pair, like the bandit methods sample and average rewards for each action. The main difference is that now there are multiple states, each acting like a different bandit problem and that the different bandit problems are interrelated.</p>
    <h4>Monte Carlo Prediction</h4>
    <p>To learn the state-value function for a given policy, given that the value of a state is the expected return starting from that state, the average of returns should converge to the expected value when more returns are observed.</p>
    <p>For instance, to estimate \(v_\pi(s)\). Each occurence of state \(s\) in an episode is called a visit, the first-visit MC method estimates the value ast the average of the returns following first visits to \(s\), the every-visit MC method averages the returns following all visits to \(s\). Both converge as the number of visits goes to infinity, but every-visit MC is less straightforward. </p>
    <ul>
    <li>First visit MC \(G\leftarrow \text{return following the 1st occurence of }s\)</li>
    <li>Every visit MC</li>
    </ul>
    <h5>Blackjack experiment</h5>
    <p>Assume that player hits (request one more card) whenever his sum is not 20 or 21. Ace can be counted as 1 or 11. In this case, the ace is suppressed to 1 if the sum is greater than 20 or 21. If the sum is 20 or 21, the player sticks (stop). This is the player policy. The dealer never hits, and only the first card is shown. </p>
    <p>The rewards of 1,-1 and 0 are given for winning, losing, and drawing respectively. The game ends when the player stops hitting.</p>
    <div style="text-align:center;">
    <img style="background-color:white;" src="../../resources/ql21_1.png" width="30%">
    <img style="background-color:white;"src="../../resources/ql21_2.png" width="30%">
    </div>
    <div style="text-align:center;">
    <img style="background-color:white;"src="../../resources/ql21_3.png" width="30%">
    <img style="background-color:white;"src="../../resources/ql21_4.png" width="30%">
    </div>
    <p>1. Although this game has complete knowledge of the environment, it would not be easy to apply DP methods, since they require the quantities \(p(s',r|s,a)\) and the expected rewards. 2. An important fact about MC methods is that the estimates for each state are independent. The estimate for one state does not build upon the estimate of any other state, as in the ase in DP. 3. The computational expense of estimating the value of a single state is independent of the number of states. This is an attractive pt of MC methods when one requires the value of only one or a subset of states.</p>

    <div style="text-align:center;">
    <img style="background-color:white;"src="../../resources/ql21_8.png" width="30%">
    <img style="background-color:white;"src="../../resources/ql21_7.png" width="30%">
    </div>
    <div style="text-align:center;">
    <img style="background-color:white;"src="../../resources/ql21_6.png" width="30%">
    <img style="background-color:white;"src="../../resources/ql21_5.png" width="30%">
    </div>
    <div class="formula">
    $
    \text{Initialize, for all }s\in\mathcal{S}, a\in\mathcal{A}(s): \\
    \quad Q(s,a)\leftarrow 
    $   
    </div>
    <h3>References</h3>
    <hr>
    </div>
    <script type="text/javascript" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
    <script type="text/javascript" src="https://zcemycl.github.io/static/eqresp.js"></script>

    
</body>
</html>
