<!DOCTYPE html>
<html>
<head>
    <meta content="text/html;charset=utf-8" http-equiv="Content-Type">
    <meta content="utf-8" http-equiv="encoding">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <meta name="viewport" content="width=device-width; initial-scale=1.0">
    <style>
      body{background-color:black;color:white;height:100%;font-family:Arial,Helvetica,sans-serif;}
      /*h3{color:#00ff2b}*/
      a{color:#337AB7;font-weight:bold;text-decoration:none;}
      pre code {
        color:white;
        border: 1px solid #999;
        display: block;
        padding: 5px 10px;
        white-space:pre-wrap;
        word-wrap:break-word;

      }
      table{width:100%}
      td,tr,th{border:1px solid white;}
      td,th{width:25%;}
      div.formula{overflow-x:scroll;border:1px solid white;}
      ::-webkit-scrollbar {
          width: 5px;
      }

      /* Track */
      ::-webkit-scrollbar-track {
          width:5px;
          -webkit-box-shadow: inset 0 0 6px rgba(0,0,0,0.3);
          -webkit-border-radius: 10px;
          border-radius: 10px;
      }

      /* Handle */
      ::-webkit-scrollbar-thumb {
          -webkit-border-radius: 10px;
          width:5px;
          border-radius: 10px;
          background: #555;
          -webkit-box-shadow: inset 0 0 6px rgba(0,0,0,0.5);
      }

      /* Handle on hover */
      ::-webkit-scrollbar-track:hover {
          width:5px;
          background: #555;
      }

      ::-webkit-scrollbar-thumb:hover {
          width:5px;
          background: orange;
      }

      /* Handle on hover */
      ::-webkit-scrollbar-track:active {
          width:5px;
          background: #555;
      }

    </style>
</head>
<body class="content wrapper">
    <div style="background-color:black; color:white; padding:0 2% 0 2%; height: 100%; flex:1;overflow: auto;">

    <h3>Markov Decision Processes</h3>
    <hr>
    <h4>Agent-Environment Interface</h4>
    <p>The learner and decision-maker = agent, the thing it interacts with = environment. The agent takes action \(A_t\) in the environment, it changes the state \(S_t\) of itself in the environment, for example, location, and the environment feedbacks the reward \(R_t\) to the agent, i.e. \(S_t \xrightarrow{A_t} S_{t+1}R_{t+1}\). </p>
    <p>Imagine that, you have a robot. The robot's action can be a plan, some non-physical output, the environment can be its body or the floor, etc. Rewards are just sth non physical, an intrinsic value to estimate how close the goal is. The agent is the algorithm, the consciousness?</p>
    <h4>Goals and Rewards</h4>
    <p>Goal is the maximization of the expected value of the cumulative sum of a received scalar signal called reward. Designing the reward function can be a challenge, it tells the robot what you want it to achieve, not how you want it achieved. </p>
    <h4>Returns</h4>
    <p>This has been discussed in introduction. Just quickly mention right here, </p>
    $$G_t=R_{t+1}+R_{t+2}+\cdots+R_T$$
    $$G_t=R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\cdots = \sum^\infty_{k=0}\gamma^k R_{t+k+1}$$
    $$G_t=\sum^{T-t-1}_{k=0}\gamma^k R_{t+k+1}$$
    <h4>Markov Property</h4>
    <p>The state is any information available to the agent. Main concern is not with designing the state signal, but with deciding with what action to take as a function of whatever state signal is available. A state signal that succeeds in retaining all relevant information is said to be Markov, or to have the Markov property. An independence of path property is defined as that all that matters is in the current state signal. </p>
    <p>The probability of responses (the environment dynamics), </p>
    $$Pr(R_{t+1}=r,S_{t+1}=s'|\{S_i\}_{i=0}^t,\{A_i\}_{i=0}^t,\{R\}_{i=1}^t)$$
    <p>If the state signal has the Markov property, the environment's response at \(t+1\) depends only on the state and action representations at \(t\). The environment's dynamics becomes,</p>
    $$p(s',r|s,a) = Pr(R_{t+1}=r,S_{t+1}=s'|S_t=s,A_t=a)$$
    <p>This is named as one-step dynamics of the environment.</p>
    <h4>Markov Decision Processes (MDP)</h4>
    <p>The expected rewards for state-action pairs,</p>
    $$r(s,a)=\mathbb{E}[R_{t+1}|S_t=s,A_t=a]=\sum_{r\in\mathfrak{R}}r\sum_{s'\in\mathfrak{S}}p(s',r|s,a)$$
    <p>The state-transition probabilities,</p>
    $$p(s'|s,a)=Pr(S_{t+1}=s'|S_t=s,A_t=a)=\sum_{r\in\mathfrak{R}}p(s',r|s,a)=\mathfrak{P}^a_{ss'}$$
    <p>The expected rewards for state-action-next-state triples,</p>
    $$r(s,a,s')=\mathbb{E}[R_{t+1}|S_t=s,A_t=a,S_{t+1}=s']=\frac{\sum_{r\in\mathfrak{R}}rp(s',r|s,a)}{p(s'|s,a)}=\mathfrak{R}^a_{ss'}$$
    <p>A transition graph is a useful way to summarize the dynamics of a finite MDP, mainly the last two quantities, the state transition probabilities and rewards for triples. It has two features, state node for each possible state, and an action node for each state-action pair. (may relate to message-passing graphical model)</p>
    <h4></h4>
    <p></p>
    <h4></h4>
    <p></p>
    <h3>References</h3>
    <hr>
    </div>
    <script type="text/javascript" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
    <script type="text/javascript" src="https://zcemycl.github.io/static/eqresp.js"></script>

    
</body>
</html>
