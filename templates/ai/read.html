<!DOCTYPE html>
<html>
<head>
    <meta content="text/html;charset=utf-8" http-equiv="Content-Type">
    <meta content="utf-8" http-equiv="encoding">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <meta name="viewport" content="width=device-width; initial-scale=1.0">
    <style>
      body{background-color:black;color:white;height:100%;font-family:Arial,Helvetica,sans-serif;}
      /*h3{color:#00ff2b}*/
      a{color:#337AB7;font-weight:bold;text-decoration:none;}
      pre code {
        color:white;
        border: 1px solid #999;
        display: block;
        padding: 5px 10px;
        white-space:pre-wrap;
        word-wrap:break-word;

      }
      table{width:100%}
      td,tr,th{border:1px solid white;}
      td,th{width:25%;}
      div.formula{overflow-x:scroll;border:1px solid white;}
      ::-webkit-scrollbar {
          width: 5px;
      }

      /* Track */
      ::-webkit-scrollbar-track {
          width:5px;
          -webkit-box-shadow: inset 0 0 6px rgba(0,0,0,0.3);
          -webkit-border-radius: 10px;
          border-radius: 10px;
      }

      /* Handle */
      ::-webkit-scrollbar-thumb {
          -webkit-border-radius: 10px;
          width:5px;
          border-radius: 10px;
          background: #555;
          -webkit-box-shadow: inset 0 0 6px rgba(0,0,0,0.5);
      }

      /* Handle on hover */
      ::-webkit-scrollbar-track:hover {
          width:5px;
          background: #555;
      }

      ::-webkit-scrollbar-thumb:hover {
          width:5px;
          background: orange;
      }

      /* Handle on hover */
      ::-webkit-scrollbar-track:active {
          width:5px;
          background: #555;
      }

    </style>
</head>
<body class="content wrapper">
    <div style="background-color:black; color:white; padding:0 2% 0 2%; height: 100%; flex:1;overflow: auto;">

    <h3>Multi-arm Bandits</h3>
    <hr>
    <h4>Action-Value Methods</h4>
    <p>The true value of action \(a\) as \(q(a)\), and the estimated value of t-th time step as \(Q_t(a)\). With the sample-average method, the true value of an action is the mean reward received when that action is selected. </p>
    $$Q_t(a) = \frac{R_1+R_2+\cdots+R_{N_t(a)}}{N_t(a)}$$
    <p>Simplest action selection rule is with the highest estimated action value, the greedy action is given by,</p>
    $$A_t = \arg\max_aQ_t(a)$$
    <p>Alternative is to introduce \(\epsilon\) which is a small probability to select randomly from amongst all the actions with equal probability. This is called \(\epsilon-greedy\) method.</p>
    <h5>Experiment</h5>
    <div style="text-align:center;">
    <img src="../../resources/qlbasic.png" style="background-color:white;" width="60%">
    </div>
    <p>Estimate the convergence by the settings, \(T=2000\), \(q(a)\sim\mathcal{N}(0,1)\), \(a^* = \arg\max_a q(a)\)(true action, true action value). In the real world, select greedy or not by \(\mathbb{U}(0,1)\),</p>
    $$R_t = q(A_t)+\mathcal{N}(0,1) \\
    A_t = \begin{cases}\arg\max_a Q_t(a),&\text{if }1-\epsilon\\ \mathbb{U}(0,1),&\text{if }\epsilon \end{cases}$$
    <pre><code>
def nArmBandit(qa,ep,T=1000):
    allq = 5*np.ones((10,1))
    alln = np.zeros((10,1))
    sumr = 0
    pltavgr = [0]
    for i in range(1,T):
        greedy = (np.random.rand(1)<=1-ep)
        if greedy:
            a = np.argmax(allq)
        else:
            a = np.random.choice(10)
        alln[a] += 1
        r = trueQa[a]+np.random.randn(1)
        allq[a] += r/alln[a]
        sumr += r
        pltavgr.append(sumr/i)
    return pltavgr
    </code></pre>
    <p>The \(\epsilon\)-greedy method is better when the reward variance is greater. With noisier rewards it takes more exploration to find the optimal action.</p>
    <p>Keeping track of each action and reward at a give time can lead to a problem of memory and computational requirements. Alternatively, the action value and k value are only necessary in another implementation, </p>
    $$
    \begin{align*}
    Q_{k+1}&=\frac{1}{k}\sum^k_{i=1}R_i=\frac{1}{k}(R_k+\sum^{k-1}_{i=1}R_i)\\
    &=\frac{1}{k}(R_k+(k-1)Q_k)\\
    &=Q_k+\frac{1}{k}(R_k-Q_k)=Q_k+\alpha(R_k-Q_k)
    \end{align*}
    $$
    <p>This update rule is a common form later on. NewEstimate \(\leftarrow\) OldEstimate + StepSize[Target-OldEstimate].</p>
    <p>Exponential recency weight average,</p>
    $$Q_{k+1}=(1-\alpha)^kQ_1 + \sum^k_{i=1}\alpha(1-\alpha)^{k-i}R_i$$
    <p>The sample average method guarantees to converge for only one action, but not for all choices of the sequence \(\{\alpha_k(a)\}\). Another stochastic approximation theory gives the conditions requirement to assure convergence with probability 1:</p>
    $$\sum^\infty_{k=1}\alpha_k(a) = \infty \quad\text{and}\quad \sum^\infty_{k=1}\alpha^2_k(a)\lt\infty$$
    <p>Therefore, a constant step size never converges, as the second requirement does not meet.</p>
    <h4>Optimistic Initial Values</h4>
    <p>Bias exists if not all actions have been selected at least once. Initial action values can be used as a simple way of encouraging exploration.</p>
    <h4>Upper-Confidence Bound Action Selection</h4>
    <p></p>
    <p>The greedy actions are those that look best at present. \(\epsilon\)-greedy forces the non-greedy action to be tried, but no preference for those that are uncertain.</p>
    <p>It is better to select among the non-greedy actions according to their potential based on their variance or uncertainty. So the one without being explored should be selected.</p>
    $$ A_t = \arg\max_a [ Q_t(a) + c\sqrt{\frac{\ln t}{N_t(a)}} ] $$
    where \(c=2\) is the degree of exploration.
    <h4>Gradient Bandits</h4>
    <p>Learning a numerical preference \(H_t(a)\) for each action \(a\). The larger the preference, the more often that action is taken. The relative preference of one action over another is defined as a softmax distribution (gibbs/boltzmann), </p>
    $$Pr\{A_t=a\}=\frac{e^{H_t(a)}}{\sum^n_{b=1}e^{H_t(b)}}=\pi_t(a)$$
    <p>Natural learning algorithm (from stochastic gradient ascent),</p>
    $$H_{t+1}(A_t) = H_t(A_t)+\alpha(R_t-\bar{R}_t)(1-\pi_t(A_t)) $$
    <p>For \(\forall a\neq A_t\),</p>
    $$H_{t+1}(a)=H_t(a)-\alpha(R_t-\bar{R}_t)\pi_t(a)$$
    <h3>References</h3>
    <hr>
    </div>
    <script type="text/javascript" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
    <script type="text/javascript" src="https://zcemycl.github.io/static/eqresp.js"></script>

    
</body>
</html>
