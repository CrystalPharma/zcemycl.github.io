<!DOCTYPE html>
<html>
<head>
    <meta content="text/html;charset=utf-8" http-equiv="Content-Type">
    <meta content="utf-8" http-equiv="encoding">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <meta name="viewport" content="width=device-width; initial-scale=1.0">
    <style>
      body{background-color:black;color:white;height:100%;font-family:Arial,Helvetica,sans-serif;}
      /*h3{color:#00ff2b}*/
      a{color:#337AB7;font-weight:bold;text-decoration:none;}
      pre code {
        color:white;
        border: 1px solid #999;
        display: block;
        padding: 5px 10px;
        white-space:pre-wrap;
        word-wrap:break-word;

      }
      table{width:100%}
      td,tr,th{border:1px solid white;}
      td,th{width:25%;}
      div.formula{overflow-x:scroll;border:1px solid white;}
      ::-webkit-scrollbar {
          width: 5px;
      }

      /* Track */
      ::-webkit-scrollbar-track {
          width:5px;
          -webkit-box-shadow: inset 0 0 6px rgba(0,0,0,0.3);
          -webkit-border-radius: 10px;
          border-radius: 10px;
      }

      /* Handle */
      ::-webkit-scrollbar-thumb {
          -webkit-border-radius: 10px;
          width:5px;
          border-radius: 10px;
          background: #555;
          -webkit-box-shadow: inset 0 0 6px rgba(0,0,0,0.5);
      }

      /* Handle on hover */
      ::-webkit-scrollbar-track:hover {
          width:5px;
          background: #555;
      }

      ::-webkit-scrollbar-thumb:hover {
          width:5px;
          background: orange;
      }

      /* Handle on hover */
      ::-webkit-scrollbar-track:active {
          width:5px;
          background: #555;
      }

    </style>
</head>
<body class="content wrapper">
    <div style="background-color:black; color:white; padding:0 2% 0 2%; height: 100%; flex:1;overflow: auto;">

    <h3>Policy Iteration</h3>
    <hr>
    <h4>Value Functions</h4>
    <p>Value functions (of states/state-action pairs) are to estimate how good it is for the agent to be in a given state (/to perform a given action and state).</p>
    <p>Consider \(p(r|s)\),</p>
    $$p(r|s)=\sum_a\sum_{s'}p(r,a,s'|s)=\sum_a\sum_{s'}p(r,s'|a,s)p(a|s)=\sum_a\pi(a|s)\sum_{s'}p(s',r|s,a)$$
    <p>Expected reward of a given state,</p>
    $$\sum_r rp(r|s)=\sum_a\pi(a|s)\sum_{s'}\sum_r rp(s',r|s,a)$$
    <p>Expected return when starting in s and following \(\pi\),</p>
    $$
    \begin{align*}
    v_\pi(s)&=\mathbb{E}_\pi[G_t|S_t=s]=\mathbb{E}_\pi\big[\sum^\infty_{k=0}\gamma^kR_{t+k+1}|S_t=s\big] \\
    &=\mathbb{E}_\pi\big[R_{t+1}+\gamma\sum^\infty_{k=0}\gamma^kR_{t+k+2}|S_t=s\big]\\
    &=\sum_a\pi(a|s)\sum_{s'}\sum_rp(s',r|s,a)\bigg[r+\gamma\mathbb{E}_\pi\big[\sum^\infty_{k=0}\gamma_kR_{t+k+2}|S_{t+1}=s'\big]\bigg]\\
    &=\sum_a\pi(a|s)\sum_{s',r}p(s',r|s,a)[r+\gamma v_\pi(s')]
    \end{align*}
    $$
    <p>Interestingly, you can see the value of a state given a policy is the sum of the future state value. This represents the value of the current state is related to the future states, not the previous states. This is the Bellman equation for \(v_\pi\).</p>
    <p>Similarly, </p>
    $$p(r|s,a)=\sum_{s'}p(s',r|s,a)$$
    $$\sum_rp(r|s,a) = \sum_r\sum_{s'}p(s',r|s,a)$$
    <p>The expected return starting from s, taking the action a, and following policy \(\pi\).</p>
    $$q_\pi(s,a)=\mathbb{E}_\pi[G_t|S_t=s,A_t=a]=\mathbb{E}_\pi\big[\sum^\infty_{k=0}\gamma^kR_{t+k+1}|S_t=s,A_t=a\big]\\
    =\sum_{s',r}p(s',r|s,a)(r+\gamma q_\pi(s',a'))
    $$
    <p>The backup diagram (existing in the book) shows the relationships that form the basis of the update, it transfer value information back to a state from its successor states.</p>
    <h4>Optimal Value Functions</h4>
    <p>A policy \(\pi\) is defined to be better than or equal to a policy \(\pi'\) for all states, i.e. \(\pi\geq\pi'\) if and only if \(v_\pi(s)\geq v_{\pi'}(s)\) for all \(s\in\mathfrak{S}\). This is an optimal policy. The optimal state-value function is defined as,</p>
    $$v_*(s) = \max_\pi v_\pi(s)$$
    <p>The optimal action-value function is defined as, </p>
    $$q_*(s,a)=\max_\pi q_\pi(s,a)$$
    <p>The Bellman optimality equation expresses the fact that the value of a state under an optimal policy must equal the expected return for the best action from that state,</p>
    $$v_*(s)=\max_{a\in\mathfrak{A}(s)}q_{\pi*}(s,a)=\max_a\mathbb{E}_{\pi*}[G_t|S_t=s,A_t=a]\\
    =\max_{a\in\mathfrak{A}(s)}\sum_{s',r}p(s',r|s,a)[r+\gamma v_*(s')]\\
    q_*(s,a) = \sum_{s',r}p(s',r|s,a)[r+\gamma\max_{a'}q_*(s',a')]
    $$
    <p>This solution relies on at least 3 assumptions that are rarely true in practice. (1) we accurately know the dynamics of the environment. (2) we have enough computational resources to complete the computation of the solution. (3) the Markov property.</p>
    <h4>Dynamic Programming</h4>
    <p>Dynamic programming referes to a collection of algorithms that can be used to compute optimal policies given a perfect model of the environment as a MDP.</p>
    <hr>
    <div style="text-align:center">
    <img src="https://raw.githubusercontent.com/zcemycl/Robotics/master/Reinforcemnet%20Learning/ValIter2.png" width="30%">
    <figcaption>Grid world</figcaption>
    </div>
    <h4>Steps</h4>
    <ul>
    <li>Initialize values and policies arbitrarily.</li>
    <li>Update values and policies iteratively,</li>

    <ul>
    <li>Policy Evaluation</li>
    <p>Each iteration of iterative policy evaluation backs up the value of every state once to produce the new approximate function \(V_{k+1}\). It is a full backup since they are based on all possible next states rather than on a sample next state. If \(k\rightarrow\infty\), \(V_{k}\rightarrow V_\pi\).</p>
    $$
    V_\pi(s)=\sum_a\pi(a|s)\sum_{s',r}p(s',r|s,a)(r+\gamma V_\pi(s'))  \\
    V_{k+1}(s)=\sum_a\pi(a|s)\sum_{s',r}p(s',r|s,a)(r+\gamma V_k(s')) 
    $$
    $$\max_s|V_{k+1}(s)-V_k(s)|<\theta$$
    <li>Policy Improvement</li>
    <p>The greedy policy takes the action that looks best in the short term -- after one step of lookahead -- according to \(v_\pi\).</p>
    $$
    v_{\pi'}(s) = \max_a\sum_{s',r}p(s',r|s,a)[r+\gamma v_{\pi'}(s')]\\
    \pi'(s) = \arg\max_a q_\pi(s,a)=\arg\max_a\sum_{s',r}p(s',r|s,a)(r+\gamma V_\pi(s'))
    $$
    <p>Let \(\pi\) and \(\pi'\) be any pair of deterministic policies such that, for all \(s\in\mathfrak{S}\),</p>
    $$q_\pi(s,\pi'(s))\geq v_\pi(s)$$
    <p>Strict inequality above holds for any state, the one below holds at least one state.</p>
    $$V_{\pi'}(s)\geq V_\pi(s)$$
    <p>You can prove that \(q_\pi(s,\pi'(s))=v_{\pi'}(s)\).</p>
    </ul>
    <p>For stochastic policy \(\pi=\pi(a|s)\),</p>
    $$q_\pi(s,\pi'(s)) = \sum_a\pi'(a|s)q_\pi(s,a)$$
    <h4>Policy Iteration</h4>
    <p>A sequence of monotonically improving policies and value functions,</p>
    $$\pi_0\xrightarrow{E}v_{\pi_0}\xrightarrow{I}\pi_1\xrightarrow{E}v_{\pi_1}\xrightarrow{I}\pi_2\xrightarrow{E}v_{\pi_2}\cdots v_*$$
    where \(E\) is the policy evulation and \(I\) is the policy improvement.
    <h3>References</h3>
    <hr>
    </div>
    <script type="text/javascript" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
    <script type="text/javascript" src="https://zcemycl.github.io/static/eqresp.js"></script>

    
</body>
</html>
