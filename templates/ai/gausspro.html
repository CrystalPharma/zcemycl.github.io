<!DOCTYPE html>
<html>
<head>
    <meta content="text/html;charset=utf-8" http-equiv="Content-Type">
    <meta content="utf-8" http-equiv="encoding">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <meta name="viewport" content="width=device-width; initial-scale=1.0">
    <style>
      body{background-color:black;color:white;height:100%;font-family:Arial,Helvetica,sans-serif;}
      /*h3{color:#00ff2b}*/
      a{color:#337AB7;font-weight:bold;text-decoration:none;}
      pre code {
        color:white;
        border: 1px solid #999;
        display: block;
        padding: 5px 10px;
        white-space:pre-wrap;
        word-wrap:break-word;

      }
      table{width:100%}
      td,tr,th{border:1px solid white;}
      td,th{width:25%;}
      div.formula{overflow-x:scroll;border:1px solid white;}

    </style>
</head>
<body class="content wrapper">
    <div style="background-color:black; color:white; padding:0 2% 0 2%; height: 100%; flex:1;overflow: auto;">

    <h3>Introduction of Gaussian Process</h3>
    <hr>
    <h4>Bayesian Inference</h4>
    <p>At the weight space view, the target is estimated as follows, \(\mathbb{y=f+\epsilon=\Phi^\top w+\epsilon}\), where \(\mathbb{w}\sim\mathcal{N}(0,\mathbb{\Sigma_p})\) and \(\epsilon\sim \mathcal{N}\mathbb{0,\sigma^2_nI})\). The posterior is given by,</p>
    <div class="formula">
    $$p(\mathbb{w|x,y,}\mathcal{M})=\mathcal{N}(\mathbb{w};\sigma^{-2}_n(\sigma^{-2}_n\Phi\Phi^\top+\mathbb{\Sigma_p^{-1}})^{-1}\mathbb{\Phi y},(\sigma_n^{-2}\Phi\Phi^\top+\Sigma_p^{-1})^{-1}) \\ 
    = \mathcal{N}(\mathbb{w;\sigma^{-2}_n}A^{-1}\Phi y,A^{-1})$$
    </div>
    <p>The marginal likelihood is written as, </p>
    <div class="formula">
    $$p(\mathbb{y|x,}\mathcal{M})=\mathcal{N}(\mathbb{y;0,\Phi^\top\Sigma_p\Phi+\sigma^2_nI})=\mathcal{N}(\mathbb{y;0,}K+\sigma^2_n\mathbb{I})$$
    </div>
    <p>The predictive distribution is derived as follows,</p>
    <div class="formula">
    $$p(\mathbb{y_*|x_*,x,y,}\mathcal{M})=\mathcal{N}(\mathbb{y_*};\sigma^{-2}_n\phi_*^\top(\sigma^{-2}_n\Phi\Phi^\top + \mathbb{\Sigma_p^{-1}})^{-1}\Phi\mathbb{y},\phi^\top_*(\sigma^{-2}_n\Phi\Phi^\top+\Sigma_p^{-1})^{-1}\phi_*+\sigma^2_n\mathbb{I}) \\ = \mathcal{N}(\mathbb{y_*;\sigma^{-2}_n\phi_*^\top}A^{-1}\Phi\mathbb{y},\phi^\top_*A^{-1}\phi_*+\sigma^2_n\mathbb{I})$$
    </div>
    <p>Consider the definition of \(A\) and \(K\), the posterior and predictive can be rewritten, </p>
    <div class="formula">
    \begin{align*}
    A&=\sigma^{-2}_n\Phi\Phi^\top+\Sigma_p^{-1} \\
    A\Sigma_p &= \sigma^{-2}_n\Phi\Phi^\top\Sigma_p+\mathbb{I}\\
    A\Sigma_p\Phi &= \sigma^{-2}_n\Phi\Phi^\top\Sigma_p\Phi+\Phi\\
     &= \sigma^{-2}_n\Phi(K+\sigma^2_n\mathbb{I})\\
    \Sigma_p\Phi(K+\sigma^2_n\mathbb{I})^{-1} &= \sigma^{-2}_nA^{-1}\Phi\\
    \end{align*}
    </div>
    <p>and by matrix cookbook inverse (woodbury identity), \((A+CBC^\top)^{-1}=A^{-1}-A^{-1}C(B^{-1}+C^\top A^{-1}C)^{-1}C^\top A^{-1}\),</p>
    <div class="formula">
    \begin{align*}
    A^{-1}&=(\Phi\sigma^{-2}_n\mathbb{I}\Phi^\top+\Sigma_p^{-1})^{-1} \\
    &= \Sigma_p-\Sigma_p\Phi(\Phi^\top\Sigma_p\Phi+\sigma_n^2\mathbb{I})^{-1}\Phi^\top\Sigma_p
    \end{align*}
    </div>
    <p>Therefore, the posterior is given by,</p>
    <div class="formula">
    \begin{align*}
    p(\mathbb{w|x,y,\mathcal{M}}) = \mathcal{N}(  \Sigma_p\Phi(K+\sigma^2_n\mathbb{I})^{-1}y,\Sigma_p-\Sigma_p\Phi(K+\sigma_n^2\mathbb{I})^{-1}\Phi^\top\Sigma_p)
    \end{align*}
    </div>
    
    <p>The predictive becomes,</p>
    
    <div class="formula">
    \begin{align*}
    &p(\mathbb{y_*|x_*,x,y,}\mathcal{M})=\mathcal{N}(\mathbb{y_*;}\phi_*^\top\Sigma_p\Phi(K+\sigma^2_n\mathbb{I})^{-1}\mathbb{y},\phi_*^\top\Sigma_p\phi_*-\phi_*^\top\Sigma_p\Phi(K+\sigma^2_n\mathbb{I})^{-1}\Phi^\top\Sigma_p\phi_*+\sigma^2_n\mathbb{I}) \\
    &=\mathcal{N}(\mathbb{y_*};K(\mathbb{x_*,x})(K+\sigma^2_n\mathbb{I})^{-1}\mathbb{y},K(\mathbb{x_*,x_*})+\sigma^2_n\mathbb{I}-K(\mathbb{x_*,x})(K+\sigma^2_n\mathbb{I})^{-1}K(\mathbb{x,x_*}))
    \end{align*}
    </div>

    <h4>Function Space View</h4>
    <p>Among all the expressions above, only the marginal likelihood seems to give the initial insight over the functional space given the inputs. It can be interpreted as, the model selection is determined by marginal likelihood \(p(\mathbb{y|x,}\mathcal{M})\), the parameters are tuned by posterior maximization \(p(\mathbb{w|x,y,}\mathcal{M})\), and the prediction is made by \(p(\mathbb{y_*|x_*,x,y,}\mathcal{M})\).</p>

    <h3>References</h3>
    <hr>
    <ol>
    <li><a href="https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf">The Matrix Cookbook</a></li>
    <li><a href="http://www.gaussianprocess.org/gpml/">Gaussian Processes for Machine Learning</a></li>
    </ol>
    </div>
    <script type="text/javascript" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
    <script type="text/javascript" src="https://zcemycl.github.io/static/eqresp.js"></script>

    
</body>
</html>
