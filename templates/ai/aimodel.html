<!DOCTYPE html>
<html>
<head>
    <meta content="text/html;charset=utf-8" http-equiv="Content-Type">
    <meta content="utf-8" http-equiv="encoding">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <meta name="viewport" content="width=device-width; initial-scale=1.0">
    <style>
      body{background-color:black;color:white;height:100%;font-family:Arial,Helvetica,sans-serif;}
      /*h3{color:#00ff2b}*/
      a{color:#337AB7;font-weight:bold;text-decoration:none;}
      pre code {
        color:white;
        border: 1px solid #999;
        display: block;
        padding: 5px 10px;
        white-space:pre-wrap;
        word-wrap:break-word;

      }
      table{width:100%;table-layout:fixed;word-wrap:break-word;}
      td,tr,th{border:1px solid white;}
      div.formula{overflow-x:scroll;border:1px solid white;}

    </style>
</head>
<body class="content wrapper">
    <div style="background-color:black; color:white; padding:0 2% 0 2%; height: 100%; flex:1;overflow: auto;">

    <h3>Neural Network Packages</h3>
    <hr>
    <h4>Image Dimension</h4>
    <table>
    <tr>
        <th></th>
        <th>Keras</th>
        <th>Tensorflow</th>
        <th>PyTorch</th>
    </tr>
    <tr>
        <td>Dimension</td>
        <td>(H,W,C,B)</td>
        <td>(B,H,W,C)</td>
        <td>(B,C,H,W)</td>
    </tr>
    </table>
    <h4></h4>
    <p>Procedures to replicate models: See input parser &#8594 main &#8594 network &#8594 build layer by layer with fake input of certain dimension &#8594 write loss &#8594 complete dataloader &#8594 optional add tensorboard &#8594 complete procedure, train and deploy.</p>
    <h4>Installation</h4>   
    <p>Installation of Pytorch is easier than that of Tensorflow-gpu, you should alert to the fact that, tensorflow does not usually support the latest version of cuda. For example, now pytorch supports cuda 11, but tensorflow only supports cuda 10.1.</p>
    <h5>Tensorflow</h5>
    <p>Sometimes, it is complicated to get this working with gpu, please pay attention to the software requirements of <code>tensorflow-gpu</code>.</p>
    <p>For Tensorflow-gpu == 2.3.0 and ubuntu 18.04 system, it requires cuda 10.1, libcudnn 7.6, nvidia-gpu driver &gt 418.x, etc. See details from the official website <a href="https://www.tensorflow.org/install/gpu">Tensorflow-GPU</a>.</p>
    <pre><code>
# Add NVIDIA package repositories
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-repo-ubuntu1804_10.1.243-1_amd64.deb
sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub
sudo dpkg -i cuda-repo-ubuntu1804_10.1.243-1_amd64.deb
sudo apt-get update
wget http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb
sudo apt install ./nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb
sudo apt-get update

# Install NVIDIA driver
sudo apt-get install --no-install-recommends nvidia-driver-450
# Reboot. Check that GPUs are visible using the command: nvidia-smi

# Install development and runtime libraries (~4GB)
sudo apt-get install --no-install-recommends \
    cuda-10-1 \
    libcudnn7=7.6.5.32-1+cuda10.1  \
    libcudnn7-dev=7.6.5.32-1+cuda10.1


# Install TensorRT. Requires that libcudnn7 is installed above.
sudo apt-get install -y --no-install-recommends libnvinfer6=6.0.1-1+cuda10.1 \
    libnvinfer-dev=6.0.1-1+cuda10.1 \
    libnvinfer-plugin6=6.0.1-1+cuda10.1
    </code></pre>
    <p>Common issues when installing is, </p>
    <ol>
    <li>Why <code>nvidia-smi</code> and <code>nvcc --version</code> show different cuda version? Has the cuda 10.1 been installed properly? Answer: trust the version from <code>nvcc</code>.</li>
    <li>Installed tensorflow gpu, but cannot detect gpu? Answer, check if the software requirements are satisfied. Either reinstall tensorflow-gpu or cuda. <a href="https://medium.com/lsc-psd/tensorflow-2-1-doesnt-seem-to-see-my-gpu-even-though-cuda-10-1-with-solution-7b44297843a">TensorFlow 2.1 doesnâ€™t recognize my GPU,though Cuda 10.1. (with Solution)</a></li>
    <li>Error message: <code>W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.0'; dlerror: libcudart.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64</code>, what does it mean? Answer: this may indicate your tensorflow-gpu requires cuda 10.0, but you only has 10.1. This is a version mismatch. Upgrade or downgrade either your tensorflow-gpu or cuda.</li>
    </ol>
    <h5>PyTorch</h5>
    <p>PyTorch installation is simpler. It only requires to have the proper cuda version. Please refer to their official website <a href="https://pytorch.org/">PyTorch Distribution</a>.</p>
    <h4>About Convolution</h4>    
    <div class="formula">
    $$n_{i+1} = \frac{n_i+2p-d(f-1)-1}{s}+1$$
    </div>
    <p>The main implementation difference betweem tensorflow and pytorch is that, whether or not the option of 'same' padding is supported.</p>
    <p>In tensorflow implementation, if padding='SAME' and stride=2 appear, it usually means the size of the input is cut in half, \(n_{i+1}=\frac{n_i}{2}\). In that case, assume that \(d=1\), the pytorch implementation should have \(p=\frac{f-2}{2}\). Even if \(f\) is odd, it does not matter, causing .5. Just round it down, for example, \(f=5,p=2\) and \(f=3,p=1\).</p>
    <p>If padding='SAME' and stride=1 appear, it means \(n_{i+1}=n_i\)</p>

    <h4>Syntax Comparison</h4>
    <table>
    <tr>
        <th>tf</th>
        <th>torch</th>
        <th>tf</th>
        <th>torch</th>
        <th>tf</th>
        <th>torch</th>
    </tr>
    <tr>
        <td><code>tf.layers.conv2d(x,f,k,s,p)</code></td>
        <td><code>nn.Conv2d(f_,f,k,s,p)(x)</code></td>
        <td><code>tf.reduce_sum</code></td>
        <td><code>torch.sum</code></td>
        <td><code>tf.reduce_mean</code></td>
        <td><code>tf.mean</code></td>
    </tr>
    <tr>
        <td><code>tf.reshape(x,)</code></td>
        <td><code>x.view(,)</code></td>
        <td><code>tf.layers.dense</code></td>
        <td><code>torch.Linear</code></td>
        <td><code>tf.image.resize_nearest_neighbor</code></td>
        <td><code>F.interpolate</code></td>
    </tr>
    <tr>
        <td><code>tf.gradients</code></td>
        <td><code>torch.autograd.gradients</code></td>
        <td><code>tf.clip_by_value</code></td>
        <td><code>torch.clamp</code></td>
        <td><code>tf.concat</code></td>
        <td><code>torch.cat</code></td>
    </tr>
    </table>
    <h4>Feature Extractions from pretrained network</h4>
    <pre><code>
def _initializeVGG(self,pretrained,freeze):
    encmodel = models.vgg16(pretrained=pretrained)
    if freeze:
        for child in encmodel.children():
            for param in child.parameters():
                param.requires_grad = False
    features = list(encmodel.features)[:31]
    self.features = nn.ModuleList(features)

    </code></pre>
    <h4>Custom Kernel</h4>
    <pre><code>
def constant_kernel(self,shape,value=1,diag=False,
        flip=False,trainable=False):
    if not diag:
        k = nn.Parameter(torch.ones(shape)*value,requires_grad=trainable)
    else:
        w = torch.eye(shape[2],shape[3])
        if flip:
            w = torch.reshape(w,(1,shape[2],shape[3]))
            w = w.flip(0,1)
        w = torch.reshape(w,shape)
        k = nn.Parameter(w,requires_grad=trainable)
    return k

def context_conv2d(self,t,dim=1,size=7,diag=False,
        flip=False,stride=1,trainable=False):
    N,C,H,W = t.size(0),t.size(1),t.size(2),t.size(3)
    in_dim = C
    size = size if isinstance(size,(tuple,list)) else [size,size]
    stride = stride if isinstance(stride,(tuple,list)) else [1,stride,stride,1]
    shape = [dim,in_dim,size[0],size[1]]
    w = self.constant_kernel(shape,diag=diag,flip=flip,trainable=trainable)
    pad = ((np.array(shape[2:])-1)/2).astype(int)
    conv = nn.Conv2d(1,1,shape[2:],1,list(pad),bias=False)
    conv.weight = w
    conv.to(self.device);
    return conv(t)

    </code></pre>
    <h4>Stacking Layers</h4>
    <p>With <code>nn.ModuleList</code> and <code>nn.Sequential</code>,</p>
    <pre><code>
class pix2pixDMap(nn.Module):
    def __init__(self,units=64):
        super(pix2pixDMap,self).__init__()
        self.layers = []
        f = units
        nodelist = [6,f,2*f,4*f,8*f,1]
        for idx in range(len(nodelist)-1):
            norm = False if idx==0 or idx==len(nodelist)-2 else True
            act = 'leaky' if idx != len(nodelist)-2 else None
            kernel = 3 if idx >= len(nodelist)-2 else 4
            stride = 1 if idx >= len(nodelist)-2 else 2
            self.layers.append(self._discriminate(nodelist[idx],
                nodelist[idx+1],k=kernel,s=stride,
                batchnorm=norm,activation=act))
        self.layers = nn.ModuleList(self.layers)
    def _discriminate(self,in_,out,k=4,s=2,p=1,
            batchnorm=True,activation='leaky'):
        block = [nn.Conv2d(in_,out,k,s,p)]
        if batchnorm:
            block.append(nn.BatchNorm2d(out))
        if activation == 'leaky':
            block.append(nn.LeakyReLU(.2))
        return nn.Sequential(*block)
    def forward(self,x):
        for idx,layer in enumerate(self.layers):
            x = layer(x)
        x = torch.sigmoid(x)
        return x

    </code></pre>
    <h3>References</h3> 
    <hr>
    </div>
    <script type="text/javascript" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
    <script type="text/javascript" src="https://zcemycl.github.io/static/eqresp.js"></script>

    
</body>
</html>
