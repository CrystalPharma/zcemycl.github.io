<!DOCTYPE html>
<html>
<head>
    <meta content="text/html;charset=utf-8" http-equiv="Content-Type">
    <meta content="utf-8" http-equiv="encoding">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script type="text/javascript" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <meta name="viewport" content="width=device-width; initial-scale=1.0">
    <style>
      body{background-color:black;color:white;height:100%;font-family:Arial,Helvetica,sans-serif;}
      /*h3{color:#00ff2b}*/
      a{color:#337AB7;font-weight:bold;text-decoration:none;}
      div.formula{overflow-x:scroll;border:1px solid white;}
    </style>
<script type="text/javascript">
window.MathJax = {
  jax: ["input/TeX","output/CommonHTML"],
  CommonHTML: { linebreaks: {automatic: true}},
  SVG: { linebreaks: {automatic: true}},
  "HTML-CSS": { linebreaks: {automatic: true}},
  extensions: ["tex2jax.js", "asciimath2jax.js", "mml2jax.js", "MathMenu.js", "MathZoom.js"],
  TeX: {
    extensions: ["AMSmath.js", "AMSsymbols.js", "autoload-all.js"]
  },
  tex2jax: {
    inlineMath: [
      ['$', '$'],
      ["\\(", "\\)"]
    ],
    processEscapes: true
  },
  AuthorInit: function() {MathJax.Hub.Register.MessageHook("End Process", function (message) {
            var timeout = false, // holder for timeout id
            delay = 250; // delay after event is "complete" to run callback
            var reflowMath = function() {
              var dispFormulas = document.getElementsByClassName("formula");
              if (dispFormulas){
              for (var i=0; i<dispFormulas.length; i++){
                var dispFormula = dispFormulas[i];
                var child = dispFormula.getElementsByClassName("MathJax_Preview")[0].nextSibling.firstChild;
                var isMultiline = MathJax.Hub.getAllJax(dispFormula)[0].root.isMultiline;
                if(dispFormula.offsetWidth < child.offsetWidth || isMultiline){
                  MathJax.Hub.Queue(["Rerender", MathJax.Hub, dispFormula]);
                }
              }
            }
            };
            window.addEventListener('resize', function() {
                // clear the timeout
              clearTimeout(timeout);
              // start timing for event "completion"
              timeout = setTimeout(reflowMath, delay);
            });
          });
  }
};

(function(d, script) {
  script = d.createElement('script');
  script.type = 'text/javascript';
  script.async = true;
  script.onload = function() {
    // remote script has loaded
  };
  script.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML';
  d.getElementsByTagName('head')[0].appendChild(script);
}(document));
</script>
</head>
<body class="content wrapper">
    <div style="background-color:black; color:white; padding:0 2% 0 2%; height: 100%; flex:1;overflow: auto;">

    <h3>Generative Adversarial Networks</h3>
    <hr>
    <h4> 1. Generative Adversarial Net (GAN)  2. Deep Convolutional GAN (DCGAN)</h4>
    <p>
    The Discriminator \(D\) and Generator \(G\) play a two-player minmax game with value function \(V(G,D)\): 
    </p>
    <div class="formula">
    $$\min_G\max_D V(G,D) = \mathbf{E}_{x\sim p_{data}(x)}[\log D(x)]+\mathbf{E}_{z\sim p(z)}[\log(1-D(G(z)))]$$
    </div>
    Given sample minibatch of \(m\) noise samples \(\{z^{(n)}\}_{i=1}^m\) from noise prior \(p_g(z)\), and sample minibatch of \(m\) examples \(\{x^{(n)}\}_{i=1}^m\) from data generating distribution \(p_{data}(x)\).

    The discriminator loss is given by, 
    <div class="formula">
    $$\frac{1}{m}\sum^m_{i=1}\bigg[\log D(x^{(i)})+\log \big(1-D(G(z^{(i)}))\big)\bigg]$$
    </div>
    The generator loss is given by,
    <div class="formula">
    $$\frac{1}{m}\sum^m_{i=1}\log D(G(z^{(i)}))$$
    </div>
    Put \(\epsilon\) within each \(\log\) to avoid \(\log 0\).
    <hr>
    <h4> 3. Least Squares GAN (LSGAN)</h4>
    The discriminator loss is given by,
    <div class="formula">
    $$\min_D V(D) = \frac{1}{2}\mathrm{E}_{x\sim p_{data}(x)}[(D(x)-b)^2]+\frac{1}{2}\mathrm{E}_{z\sim p_z(z)}[(D(G(z))-a)^2]$$
    </div>
    The generator loss is minimized via,
    <div class="formula">
    $$\min_G V(G) = \frac{1}{2}\mathrm{E}_{z\sim p_z(z)}[(D(G(z))-c)^2]$$
    </div>
    Hyperparameter options: 1. \(a=-1,b=1 \text{ and }c=0\), or 2. \(a=c=-1 \text{ and }b=0\).
    <hr>
    <h4> 4. Context-Conditional GAN (CCGAN)</h4>
    Let \(\mathbf{m} \in \mathbb{R}^d\) denote to a binary mask that can be used to drop out a specified portion of an image. The generator receives as input \(\mathbf{m}\odot\mathbf{x}\) where \(\odot\) denotes element-wise multiplication. The generator outputs \(\mathbf{x_G}=G(\mathbf{m}\odot\mathbf{x},\mathbf{z})\in \mathbb{R}^d\) and the in-painted image \(\mathbf{x_I}\) is given by,
    <div class="formula">
    $$\mathbf{x_I}=(1-\mathbf{m})\odot\mathbf{x_G}+\mathbf{m}\odot\mathbf{x}$$
    </div>
    The CCGAN objective is given by,
    <div class="formula">
    $$\min_G\max_D \mathrm{E}_{\mathbf{x}\sim\mathcal{X}}[\log D(\mathbf{x})]+\mathrm{E}_{\mathbf{x}\sim\mathcal{X},\mathbf{m}\sim\mathcal{M}}[1-D(\mathbf{x_I})]$$
    </div>

    <hr>
    <h4>5. Auxiliary Classifier GAN (ACGAN)</h4>
      Another formulation: The generator \(G\) takes an input a random noise vector \(z\) and outputs an image \(X_{fake}=G(z)\). The discriminator \(D\) receives as input either a training image or a synthesized image from the generator and outputs a probability distribution \(P(S|X) = D(X)\) over possible image sources. The discriminator is trained to maximize the log-likelihood it assigns to the correct source:
    <div class="formula">
      $$L = E[\log P(S=real|X_{real})]+E[\log P(S=fake|X_{fake})]$$
    </div>
      For ACGAN, every generated sample has a corresponding class label \(c\sim p_c\) in addition to the noise \(z\). \(G\) uses both to generate images \(X_{fake}=G(c,z)\). The discriminator gives both a probability distribution over sources and a probability distribution over the class labels, \(P(S|X),P(C|X) = D(X)\). THe objective function has two parts: the log-likelihood of the correct source \(L_S\) and the log-likelihood of the correct class \(L_C\). 
    <div class="formula">
      $$L_S = E[\log P(S=real|X_{real})]+E[\log P(S=fake|X_{fake})]$$
      $$L_C = E[\log P(C=c|X_{real})]+E[\log P(C=c|X_{fake})]$$
    </div>
      \(D\) is trained to maximize \(L_S+L_C\) while \(G\) is trained on maximize \(L_C-L_S\). ACGAN learns a representation for \(z\) that is independent of class label.
    <hr>
    <h4>6. InfoGAN</h4>
    <hr>
    <h4>7. Adversarial Autoencoder</h4>
    <hr>
    <h4>8. Image-to-Image Translation (Pix2Pix)</h4>
    The objective of a conditional GAN can be expressed as,
    <div class="formula">
    $$\mathcal{L}_{cGAN}(G,D) = \mathrm{E}_{x,y}[\log D(x,y)]+\mathrm{E}_{x,z}[\log(1-D(x,G(x,z)))]$$
    </div>
    to discriminate whether the condition image is paired with the correct image. L1 loss encourages less blurring, 
    <div class="formula">
    $$\mathcal{L}_{L1}(G) = \mathrm{E}_{x,y,z}[\|y-G(x,z)\|_1]$$
    </div>
    The final objective is given by,
    <div class="formula">
    $$G^* = \arg\min_G\max_D \mathcal{L}_{cGAN}(G,D)+\lambda\mathcal{L}_{L1}(G)$$
    </div>
    <hr>
    <h4> 9. Cycle-Consistent Adversarial Network (Cycle-GAN)</h4>
    The model contains two mapping functions (generators) \(G:X\rightarrow Y\) and \(F:Y\rightarrow X\), and associated adversarial discriminators \(D_Y\) and \(D_X\). \(D_Y\) encourages \(G\) to translate \(X\) into outputs indistinguishable from domain \(Y\), and vice versa for \(D_X\) and \(F\).

    For the mapping function \(G:X\rightarrow Y\) and its discriminator \(D_Y\), the objective is expressed as,
    <div class="formula">
    $$\mathcal{L}_{GAN}(G,D_Y,X,Y)=\mathrm{E}_{y\sim p_{data}(y)}[\log D_Y(y)]+\mathrm{E}_{x\sim p_{data}(x)}[\log(1-D_Y(G(x)))]$$

    </div>
    The cycle consistency loss is given by,
    <div class="formula">
    $$\mathcal{L}_{cyc}(G,F) = \mathrm{E}_{x\sim p_{data}(x)}[\|F(G(x))-x\|_1]+\mathrm{E}_{y\sim p_{data}(y)}[\|G(F(y))-y\|_1]$$

    </div>
    The full objective is given by,
    <div class="formula">
    $$\mathcal{L}(G,F,D_X,D_Y)=\mathcal{L}_{GAN}(G,D_Y,X,Y)+\mathcal{L}_{GAN}(F,D_X,Y,X)+\lambda\mathcal{L}_{cyc}(G,F)$$
    </div>
    Aim to solve,
    <div class="formula">
    $$G^*,F^* = \arg\min_{G,F}\max_{D_X,D_Y}\mathcal{L}(G,F,D_X,D_Y)$$
    </div>
    <hr>
    <h3>References</h3>
    <hr>
    </div>

    
</body>
</html>
